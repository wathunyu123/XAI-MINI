# ส่วนที่ 1: สรุปทฤษฎีและเทคนิคสำคัญของ XAI

`XAI (Explainable AI)` คือชุดของเทคนิคและวิธีการที่ช่วยให้มนุษย์สามารถทำความเข้าใจและเชื่อถือผลลัพธ์และการตัดสินใจที่สร้างโดยโมเดล Machine Learning ได้ โดยเฉพาะโมเดลที่ซับซ้อนอย่าง Deep Learning หรือ Ensemble Models ที่ถูกเรียกว่าเป็น "Black Box"

เทคนิคสำคัญสามารถแบ่งตามขอบเขตของคำอธิบายได้เป็น 2 ประเภทหลัก ได้แก่ `Local` (อธิบายการตัดสินใจรายกรณี) และ `Global` (อธิบายพฤติกรรมโดยรวมของโมเดล)

## 1. `LIME (Local Interpretable Model-agnostic Explanations)`

หลักการทำงาน: เป็นเทคนิคแบบ Local ที่อธิบายว่าทำไมโมเดลถึงตัดสินใจแบบนั้นสำหรับ "ข้อมูลหนึ่งชิ้น" โดยการสร้างโมเดลที่เข้าใจง่าย (เช่น Linear Regression) ขึ้นมา "เลียนแบบ" พฤติกรรมของโมเดลที่ซับซ้อน ณ บริเวณใกล้เคียงกับข้อมูลชิ้นนั้น

ขั้นตอนดังนี้:

- เลือกข้อมูลที่ต้องการอธิบาย 1 ชิ้น (เช่น ภาพแมวที่โมเดลทายถูก)

- สร้างข้อมูลใหม่จำนวนมากโดยการ "ดัดแปลง" ข้อมูลต้นฉบับเล็กน้อย (เช่น ปิดบางส่วนของภาพ)

- นำข้อมูลที่ดัดแปลงไปให้โมเดล Black Box ทำนายผล

- สร้างโมเดลที่เข้าใจง่าย เช่น `Linear Regression` เพื่อเรียนรู้ความสัมพันธ์ระหว่าง `"ส่วนที่ดัดแปลง"` กับ `"ผลลัพธ์ที่ได้"`

- ผลลัพธ์คือฟีเจอร์ (หรือส่วนของภาพ) ที่มีผลต่อการตัดสินใจมากที่สุดสำหรับข้อมูลชิ้นนั้น

จุดเด่น:

- ใช้ได้กับโมเดลทุกประเภท (Tabular, Text, Image) โดยไม่จำเป็นต้องรู้โครงสร้างภายใน

- คำอธิบายออกมาในรูปแบบที่มนุษย์ตีความได้ง่าย (เช่น "โมเดลทายว่าเป็นแมวเพราะเห็นส่วนหูและหนวด")

ข้อควรระวัง:

- คำอธิบายอาจ ไม่เสถียร (Unstable) หากเปลี่ยนวิธีสุ่มสร้างข้อมูลรอบๆ เล็กน้อย ผลลัพธ์อาจเปลี่ยนไป

- คำอธิบายเป็นเพียง การประมาณค่าแบบ Local ไม่ได้สะท้อนพฤติกรรมทั้งหมดของโมเดล

## 2. `SHAP (SHapley Additive exPlanations)`

หลักการทำงาน: เป็นเทคนิคที่ใช้แนวคิดจากทฤษฎีเกม (Game Theory) ที่ชื่อว่า Shapley Values เพื่อหาว่าแต่ละฟีเจอร์มีส่วนช่วย "ผลักดัน" การทำนายให้แตกต่างจากค่าเฉลี่ยมากน้อยแค่ไหนอย่างยุติธรรม

เปรียบเทียบง่ายๆ: ถ้าการทำนายคือ "เงินรางวัล" ที่ทีมได้ SHAP จะคำนวณว่า "ผู้เล่น" (ฟีเจอร์) แต่ละคนควรได้รับส่วนแบ่งเท่าไหร่ โดยพิจารณาทุกการรวมกลุ่มที่เป็นไปได้

จุดเด่น:

มีทฤษฎีรองรับที่แข็งแกร่ง: ให้ผลลัพธ์ที่สอดคล้องและยุติธรรม (Consistency & Fairness)

ใช้ได้ทั้ง Local และ Global:

Local: อธิบายได้ว่าสำหรับข้อมูล 1 ชิ้น ฟีเจอร์ไหนมีผลบวก (ผลักดันให้ผลลัพธ์สูงขึ้น) หรือผลลบ (ผลักดันให้ผลลัพธ์ต่ำลง)

Global: สามารถนำค่า SHAP ของทุกข้อมูลมารวมกันเพื่อสร้างกราฟสรุป (เช่น Summary Plot) ที่แสดง "ความสำคัญของฟีเจอร์โดยรวม" ได้

ข้อควรระวัง:

การคำนวณอาจใช้เวลานานมากในกรณีที่มีฟีเจอร์จำนวนมาก (แต่มีอัลกอริทึมสำหรับประมาณค่า เช่น KernelSHAP, TreeSHAP)

## 3. `PDP (Partial Dependence Plot)` และ `ICE (Individual Conditional Expectation)`

หลักการทำงาน (`PDP`): เป็นเทคนิคแบบ Global ที่แสดงให้เห็นว่า "โดยเฉลี่ยแล้ว" การทำนายของโมเดลจะเปลี่ยนแปลงไปอย่างไรเมื่อเราปรับค่าของ "ฟีเจอร์หนึ่ง" ในขณะที่ฟีเจอร์อื่นๆ ถูกเฉลี่ยค่าคงไว้

หลักการทำงาน (`ICE`): เหมือนกับ `PDP` แต่แทนที่จะแสดงเส้นค่าเฉลี่ยเพียงเส้นเดียว `ICE` จะแสดง เส้นของข้อมูลแต่ละชิ้น ทำให้เห็นพฤติกรรมที่แตกต่างกันของข้อมูลแต่ละกลุ่ม (Heterogeneous Effects) ซึ่ง `PDP` อาจมองข้ามไป

จุดเด่น:

เห็นภาพรวมความสัมพันธ์ระหว่างฟีเจอร์กับผลลัพธ์ได้ง่าย

ICE ช่วยให้เห็นความสัมพันธ์ที่ซับซ้อนหรือ Interaction ที่ PDP อาจบดบังไว้

ข้อควรระวัง:

สันนิษฐานว่าฟีเจอร์ไม่ขึ้นต่อกัน (Uncorrelated) ซึ่งในความเป็นจริงอาจไม่เป็นเช่นนั้น ทำให้การตีความอาจผิดเพี้ยนได้

PDP อาจซ่อนพฤติกรรมของข้อมูลกลุ่มย่อยที่น่าสนใจเอาไว้

## 4. `Grad-CAM (Gradient-weighted Class Activation Mapping)`

หลักการทำงาน: เป็นเทคนิค สำหรับโมเดลจำแนกประเภทรูปภาพ (CNNs) โดยเฉพาะ ใช้ข้อมูล Gradient ที่ไหลย้อนกลับไปยัง Convolutional Layer สุดท้ายเพื่อสร้าง Heatmap ที่แสดงว่า "บริเวณไหนของภาพ" ที่โมเดลให้ความสำคัญมากที่สุดในการตัดสินใจ

จุดเด่น:

ให้ผลลัพธ์เป็นภาพที่เข้าใจได้ทันที (Visual Explanation)

ช่วยในการดีบักโมเดลว่าโมเดลกำลังมอง "ถูกที่" หรือไม่ (เช่น โมเดลทายว่าเป็นหมาเพราะมองที่ตัวหมา ไม่ใช่พื้นหลัง)

ข้อควรระวัง:

เป็นเทคนิค Model-specific ใช้ได้กับโมเดลที่มีสถาปัตยกรรมแบบ Convolutional เท่านั้น

ความละเอียดของ Heatmap ขึ้นอยู่กับขนาดของ Feature Map ใน Layer สุดท้าย

## 5. `Counterfactual Explanations`

หลักการทำงาน: ตอบคำถามว่า "จะต้องเปลี่ยนแปลงค่าฟีเจอร์อะไรบ้างให้น้อยที่สุด เพื่อให้ผลการทำนายเปลี่ยนไปเป็นอีกแบบ"

ตัวอย่าง: "หากลูกค้าคนนี้ถูกปฏิเสธสินเชื่อ ต้องทำอย่างไรให้ผ่าน? คำตอบอาจเป็น: หากเพิ่มรายได้ขึ้น 5,000 บาท และลดหนี้สินลง 10,000 บาท โมเดลจะทำนายว่าจะอนุมัติสินเชื่อ"

จุดเด่น:

ให้คำอธิบายที่ นำไปปฏิบัติได้ (Actionable) ผู้ใช้งานสามารถนำไปปรับปรุงเพื่อให้ได้ผลลัพธ์ที่ต้องการได้

เข้าใจง่ายมากสำหรับผู้ใช้งานทั่วไป เพราะเป็นรูปแบบของ "What-If"

ข้อควรระวัง:

Counterfactual ที่สร้างขึ้นอาจ ไม่ใช่ข้อมูลที่มีอยู่จริง (Unrealistic) เช่น การแนะนำให้คนอายุ 40 ปี เปลี่ยนเป็น 25 ปี

อาจมีคำตอบที่เป็นไปได้หลายชุด

# ส่วนที่ 2: บทเรียนด้านจริยธรรมและความยุติธรรม (Fairness) ใน XAI

การมี XAI ไม่ได้หมายความว่าระบบ AI จะดีหรือยุติธรรมเสมอไป ในทางกลับกัน มันเปิดประเด็นทางจริยธรรมที่ต้องพิจารณาอย่างรอบคอบ

XAI ไม่ใช่ยาวิเศษสำหรับแก้ปัญหา Bias:

บทเรียน: XAI เป็นเครื่องมือสำหรับ "ตรวจจับ" และ "เปิดโปง" ความลำเอียง (Bias) ในโมเดล ไม่ใช่เครื่องมือสำหรับ "แก้ไข" โดยอัตโนมัติ

ตัวอย่าง: XAI อาจแสดงให้เห็นว่าโมเดลสินเชื่อให้น้ำหนักกับ "เพศ" หรือ "เชื้อชาติ" อย่างไม่เป็นธรรม แต่มันไม่ได้บอกวิธีแก้ปัญหานั้น

ความเสี่ยง: การใช้ XAI ในทางที่ผิดอาจนำไปสู่ "Fairwashing" คือการใช้คำอธิบายที่ดูดีมาสร้างความชอบธรรมให้กับการตัดสินใจที่จริงๆ แล้วไม่ยุติธรรม

ความโปร่งใส (Transparency) vs. ความเป็นส่วนตัว (Privacy):

บทเรียน: การอธิบายการทำงานของโมเดลอย่างละเอียดเกินไปอาจเปิดเผยข้อมูลส่วนบุคคลที่ละเอียดอ่อนที่อยู่ในชุดข้อมูลฝึกสอน (Training Data)

ตัวอย่าง: โมเดลทางการแพทย์ที่อธิบายว่าทำไมถึงวินิจฉัยโรคหายากในผู้ป่วยรายหนึ่ง อาจเปิดเผยข้อมูลที่สามารถระบุตัวตนของผู้ป่วยรายนั้นได้

สิ่งที่ต้องทำ: ต้องหาจุดสมดุลระหว่างการให้คำอธิบายที่เป็นประโยชน์กับการปกป้องข้อมูลส่วนบุคคล

"คำอธิบายที่ดี" ขึ้นอยู่กับผู้รับสาร:

บทเรียน: คำอธิบายที่เหมาะกับ Data Scientist (เช่น SHAP plot) อาจไม่เหมาะกับผู้ใช้งานทั่วไป (เช่น แพทย์ หรือผู้ขอสินเชื่อ)

บริบทสำคัญที่สุด: คำอธิบายต้องถูกออกแบบให้สอดคล้องกับความต้องการและระดับความเข้าใจของผู้รับสารแต่ละกลุ่ม คำอธิบายสำหรับผู้ขอสินเชื่อควรเป็นแบบ Counterfactual ("คุณต้องทำอะไรเพิ่ม") มากกว่าจะเป็นกราฟเทคนิค

ความเสี่ยงของการเชื่อมั่นในคำอธิบายมากเกินไป (Over-trust):

บทเรียน: คำอธิบายจากเทคนิคอย่าง LIME หรือ SHAP เป็นเพียง "การประมาณค่า" พฤติกรรมของโมเดล ไม่ใช่ความจริงทั้งหมด การมีคำอธิบายที่ดูน่าเชื่อถืออาจทำให้คนเราเชื่อมั่นในโมเดลที่ผิดพลาดได้ง่ายขึ้น

ตัวอย่าง: Grad-CAM อาจชี้ไปที่บริเวณที่สมเหตุสมผลบนภาพ แต่เหตุผลที่แท้จริงที่โมเดลใช้ตัดสินใจอาจซับซ้อนกว่านั้น หรืออาจมีปัจจัยรบกวน (Spurious Correlation) ซ่อนอยู่

ความรับผิดชอบ (Accountability) และสิทธิในการร้องเรียน (Recourse):

บทเรียน: เป้าหมายสูงสุดของ XAI ในระบบที่ส่งผลกระทบสูง (High-stakes) คือการสร้าง ความรับผิดชอบ

เมื่อโมเดลตัดสินใจผิดพลาด เราต้องสามารถใช้คำอธิบายเพื่อสืบหาต้นตอของปัญหาได้

ที่สำคัญคือ ต้องให้ สิทธิในการร้องเรียน (Recourse) แก่ผู้ที่ได้รับผลกระทบ โดยใช้คำอธิบาย (โดยเฉพาะ Counterfactuals) เป็นแนวทางว่าพวกเขาสามารถเปลี่ยนแปลงอะไรได้บ้างเพื่อแก้ไขผลลัพธ์นั้น

สรุป: เทคนิค XAI เป็นเครื่องมือที่ทรงพลังในการสร้างความโปร่งใสและน่าเชื่อถือให้กับ AI แต่การนำไปใช้ต้องทำควบคู่ไปกับการตระหนักถึงข้อจำกัดทางเทคนิคและผลกระทบทางจริยธรรมเสมอ เพื่อให้แน่ใจว่าเรากำลังสร้างเทคโนโลยีที่ยุติธรรมและเป็นประโยชน์ต่อสังคมอย่างแท้จริง