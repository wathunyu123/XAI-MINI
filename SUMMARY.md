# ทฤษฎีและเทคนิคสำคัญของ Explainable AI (XAI)
ภาพรวมระเบียบวิธีและข้อพิจารณาด้านจริยธรรม

---

## **บทคัดย่อ**

Explainable AI (XAI) เป็นกลุ่มของระเบียบวิธี (methodologies) ที่ออกแบบมาเพื่อให้การตัดสินใจของโมเดลการเรียนรู้ของเครื่องที่ซับซ้อน ซึ่งมักถูกเรียกว่า "กล่องดำ" (black boxes) สามารถตีความและทำความเข้าใจได้โดยผู้มีส่วนได้ส่วนเสีย เอกสารฉบับนี้สรุประเบียบวิธีหลักของ XAI ในเชิงวิชาการ โดยจำแนกตามขอบเขตของคำอธิบาย (local vs. global) และการขึ้นกับโมเดล (model-agnostic vs. model-specific) เนื้อหาจะวิเคราะห์ระเบียบวิธีสำคัญ ได้แก่ LIME, SHAP, PDP/ICE, Grad-CAM, Counterfactual Explanations **รวมถึงแนวทางใหม่สำหรับ Large Language and Multimodal Models (LLMs)** โดยพิจารณาจากหลักการทำงาน จุดเด่น และข้อจำกัด นอกจากนี้ยังได้อภิปรายถึงข้อพิจารณาที่สำคัญด้านจริยธรรมและความยุติธรรมที่เกิดจากการประยุกต์ใช้ XAI เช่น การตรวจจับความลำเอียง ความเป็นส่วนตัวของข้อมูล และความรับผิดชอบ บทสรุปนี้ทำหน้าที่เป็นเอกสารอ้างอิงพื้นฐานสำหรับความเข้าใจในภูมิทัศน์ปัจจุบันของการเรียนรู้ของเครื่องที่สามารถตีความได้

---

## **1. บทนำสู่ Explainable AI (XAI)**

ความซับซ้อนที่เพิ่มขึ้นของโมเดลการเรียนรู้ของเครื่อง เช่น โครงข่ายประสาทเทียมเชิงลึก (Deep Neural Networks) และแบบจำลองรวม (Ensemble Methods) ได้นำไปสู่การแลกเปลี่ยนระหว่างความแม่นยำในการทำนายและความโปร่งใสของโมเดล XAI มีเป้าหมายเพื่อลดช่องว่างนี้ โดยการจัดหากลไกในการทำความเข้าใจ ตรวจสอบ และไว้วางใจในการตัดสินใจของอัลกอริทึม ระเบียบวิธีต่างๆ สามารถจำแนกได้ตาม 2 แกนหลัก คือ **ขอบเขต** (scope: Local vs. Global) และ **การขึ้นกับโมเดล** (Model Dependency: Model-agnostic vs. Model-specific)

---

## **2. ระเบียบวิธีหลักใน Explainable AI**

### **2.1. LIME (Local Interpretable Model-agnostic Explanations)**
*   **หลักการทำงาน:** LIME เป็นเทคนิคแบบ `model-agnostic` ที่อธิบายการทำนายผลรายกรณี โดยการประมาณพฤติกรรมของโมเดลกล่องดำในบริเวณใกล้เคียงกับข้อมูลที่สนใจอธิบาย ผ่านการสร้างชุดข้อมูลที่ถูกปรับแก้ (Perturbed Dataset) รอบๆ ข้อมูลนั้น แล้วนำไปให้โมเดลเดิมทำนายผล จากนั้นจึงฝึกสอนโมเดลตัวแทนที่ตีความได้ง่าย (Interpretable Surrogate Model) เช่น linear regression ด้วยข้อมูลที่สร้างขึ้นในบริเวณนั้น [1]
*   **จุดเด่น:** จุดเด่นหลักคือความเป็น `model-agnostic` ทำให้สามารถประยุกต์ใช้ได้กับโมเดลหลากหลายสถาปัตยกรรม และคำอธิบายที่ได้ก็อยู่ในรูปแบบที่มนุษย์เข้าใจง่าย
*   **ข้อจำกัด:** ความเสถียรของคำอธิบายอาจเปลี่ยนแปลงได้ตามกลยุทธ์การสร้างข้อมูลที่ถูกปรับแก้ และความน่าเชื่อถือของคำอธิบาย (Fidelity) นั้นจำกัดอยู่แค่ในบริเวณใกล้เคียง (Local) ซึ่งอาจไม่สะท้อนพฤติกรรมโดยรวมของโมเดล

### **2.2. SHAP (SHapley Additive exPlanations)**
*   **หลักการทำงาน:** SHAP ตั้งอยู่บนพื้นฐานของทฤษฎีเกมเชิงสหกรณ์ (Cooperative Game Theory) โดยจะกำหนดค่า Shapley (Shapley Value) ให้กับแต่ละฟีเจอร์ ซึ่งแสดงถึงผลงานส่วนเพิ่ม (Marginal Contribution) ของฟีเจอร์นั้นต่อการทำนายผล โดยเฉลี่ยจากการรวมกลุ่มฟีเจอร์ที่เป็นไปได้ทั้งหมด SHAP จึงเป็นกรอบการทำงานที่เป็นเอกภาพซึ่งรับประกันคุณสมบัติที่พึงประสงค์ เช่น ความแม่นยำในระดับ Local, การจัดการข้อมูลที่หายไป, และความสอดคล้อง (Consistency) [2]
*   **จุดเด่น:** มีรากฐานทางทฤษฎีที่แข็งแกร่ง ทำให้มั่นใจได้ว่าการกระจายผลของฟีเจอร์เป็นไปอย่างยุติธรรม สามารถให้คำอธิบายได้ทั้งในระดับ Local สำหรับรายกรณี และระดับ Global โดยการรวมค่า SHAP ของข้อมูลทั้งหมด
*   **ข้อจำกัด:** การคำนวณค่า Shapley ที่แม่นยำมี ความซับซ้อนระดับ NP-hard ทำให้ใช้เวลาประมวลผลสูงเมื่อมีจำนวนฟีเจอร์มาก อย่างไรก็ตาม ได้มีการพัฒนาอัลกอริทึมประมาณค่าที่มีประสิทธิภาพสำหรับโมเดลบางประเภท (เช่น `TreeSHAP` สำหรับโมเดลตระกูลต้นไม้) เพื่อลดปัญหานี้

### **2.3. PDP (Partial Dependence Plots) และ ICE (Individual Conditional Expectation) Plots**
*   **หลักการทำงาน:** PDP เป็นเทคนิคระดับ global ที่แสดงผลกระทบส่วนเพิ่ม (Marginal Effect) ของฟีเจอร์หนึ่งหรือสองฟีเจอร์ต่อผลการทำนาย โดยการหาค่าเฉลี่ยของผลการทำนายภายใต้การกระจายตัวของฟีเจอร์อื่นๆ [3] ในขณะที่ ICE Plot จะแยกแสดงเส้น PDP ของข้อมูลแต่ละตัว ทำให้สามารถมองเห็นผลกระทบที่แตกต่างกันในแต่ละกลุ่ม (Heterogeneous Effects) ซึ่งอาจถูกบดบังโดยเส้นค่าเฉลี่ยของ PDP ได้ [4]
*   **จุดเด่น:** ทั้งสองเทคนิคเป็นวิธีที่เข้าใจง่ายในการแสดงภาพความสัมพันธ์ระหว่างฟีเจอร์กับผลลัพธ์ โดยเฉพาะ ICE Plot ที่มีประสิทธิภาพในการค้นหาปฏิสัมพันธ์ระหว่างฟีเจอร์และพฤติกรรมเฉพาะกลุ่มย่อย
*   **ข้อจำกัด:** มีข้อสันนิษฐานที่สำคัญคือ **ความเป็นอิสระระหว่างฟีเจอร์ (Feature Independence)** หากฟีเจอร์มีความสัมพันธ์กัน แผนภาพอาจสร้างจุดข้อมูลที่ไม่สมจริงและนำไปสู่การตีความที่ผิดพลาดได้

### **2.4. Grad-CAM (Gradient-weighted Class Activation Mapping)**
*   **หลักการทำงาน:** Grad-CAM เป็นเทคนิคแบบ `model-specific` สำหรับโครงข่ายประสาทเทียมแบบคอนโวลูชัน (CNNs) โดยใช้ค่าเกรเดียนต์ (gradients) ของคลาสเป้าหมายเทียบกับ Feature Map ของชั้นคอนโวลูชันสุดท้าย เพื่อสร้างแผนที่ระบุตำแหน่ง (Localization Map) ที่เน้นให้เห็นบริเวณที่สำคัญในภาพอินพุตสำหรับการทำนายคลาสนั้นๆ [5]
*   **จุดเด่น:** ให้คำอธิบายเชิงภาพ (Visual Explanation) ที่มีความละเอียดสูงและจำแนกตามคลาสได้ดี ทำให้มีประโยชน์อย่างยิ่งในการดีบักและตรวจสอบว่าโมเดลโฟกัสถูกจุดหรือไม่
*   **ข้อจำกัด:** การใช้งานจำกัดอยู่เฉพาะกับโมเดลที่ใช้เกรเดียนต์เป็นหลัก (เช่น CNNs) และคำอธิบายจะถูกจำกัดอยู่แค่ในชั้นเลเยอร์เดียว ซึ่งอาจไม่สามารถจับความซับซ้อนทั้งหมดของกระบวนการตัดสินใจได้

### **2.5. Counterfactual Explanations**
*   **หลักการทำงาน:** คำอธิบายเชิงเปรียบเทียบหา "การเปลี่ยนแปลงที่น้อยที่สุด" ที่จำเป็นต่อค่าฟีเจอร์ของข้อมูลหนึ่งๆ เพื่อให้ผลการทำนายของโมเดลเปลี่ยนไปเป็นผลลัพธ์ที่กำหนดไว้ล่วงหน้า เป็นการตอบคำถามว่า "จะต้องเปลี่ยนแปลงอะไรบ้างเพื่อให้ผลลัพธ์กลายเป็น Y แทนที่จะเป็น X?" [6]
*   **จุดเด่น:** เป็นคำอธิบายที่เน้นผู้ใช้เป็นศูนย์กลางและให้แนวทางที่สามารถนำไปปฏิบัติได้จริง (Actionable Recourse) ซึ่งมีความสำคัญอย่างยิ่งในโดเมนต่างๆ เช่น การอนุมัติสินเชื่อ หรือการวินิจฉัยทางการแพทย์
*   **ข้อจำกัด:** คำอธิบายที่สร้างขึ้นอาจขาดความสมจริงหรือไม่สอดคล้องกับการกระจายตัวของข้อมูลฝึกสอน นอกจากนี้ อาจเกิดปัญหาความซ้ำซ้อนของคำตอบ (Multiplicity) ซึ่งสร้างความคลุมเครือได้

### **2.6. เทคนิคสำหรับ Large Language and Multimodal Models (LLMs)**
การเติบโตของ LLMs เช่น GPT-4 หรือ LLaMA ได้นำมาซึ่งความท้าทายใหม่ๆ ในการตีความ เนื่องจากสถาปัตยกรรมแบบ Transformer ที่ซับซ้อนและการทำงานกับข้อมูลที่ไม่มีโครงสร้าง (unstructured data) เช่น ข้อความและรูปภาพ เทคนิค XAI สำหรับ LLMs จึงมุ่งเน้นไปที่การทำความเข้าใจกลไกภายในและพฤติกรรมการสร้างผลลัพธ์

*   **หลักการทำงาน:**
    *   **Attention-based Explanation:** เนื่องจากสถาปัตยกรรม Transformer มีกลไก `Attention` เป็นหัวใจสำคัญ เทคนิคกลุ่มนี้จึงใช้ **Attention weights** ในการแสดงให้เห็นว่าโมเดลให้น้ำหนักกับส่วนใดของอินพุต (เช่น คำใดในประโยค หรือส่วนใดของภาพ) เมื่อสร้างผลลัพธ์ออกมาแต่ละส่วน [8]
    *   **Chain-of-Thought (CoT) & Self-Explanation:** เป็นแนวทางที่ให้ LLM สร้างคำอธิบายด้วยตัวเอง โดยการออกแบบ `prompt` เพื่อสั่งให้โมเดลอธิบายเหตุผลหรือกระบวนการคิดเป็นขั้นตอน (Chain-of-Thought) ก่อนที่จะให้คำตอบสุดท้าย วิธีนี้ช่วยให้เห็นตรรกะภายในของโมเดลในการได้มาซึ่งผลลัพธ์นั้น [9]
    *   **Influence Functions:** เป็นเทคนิคที่ใช้ในการระบุว่าข้อมูลฝึกสอน (training data) ชิ้นใดมีอิทธิพลต่อการทำนายผลของข้อมูลทดสอบ (test instance) ชิ้นหนึ่งๆ มากที่สุด ช่วยให้สามารถติดตามกลับไปดูต้นตอของพฤติกรรมที่ไม่พึงประสงค์ได้

*   **จุดเด่น:**
    *   `Attention-based methods` ให้คำอธิบายที่ละเอียดในระดับโทเค็น (token-level) และสอดคล้องกับสถาปัตยกรรมของโมเดลโดยตรง
    *   `Chain-of-Thought` สร้างคำอธิบายในรูปแบบภาษาธรรมชาติที่มนุษย์เข้าใจได้ง่ายมาก และไม่ต้องแก้ไขโครงสร้างภายในของโมเดล

*   **ข้อจำกัด:**
    *   **Attention ไม่ใช่ Explanation เสมอไป:** มีงานวิจัยที่ชี้ให้เห็นว่า Attention weights อาจไม่ได้สะท้อนความสำคัญของฟีเจอร์อย่างแท้จริงเสมอไป [10]
    *   **ความน่าเชื่อถือของ Self-Explanation:** คำอธิบายที่ LLM สร้างขึ้นอาจเป็นเพียงการ "หาเหตุผลเข้าข้างตัวเอง" (post-hoc rationalization) มากกว่าจะเป็นการสะท้อนกระบวนการคิดที่แท้จริง และอาจถูกชักจูงให้สร้างคำอธิบายที่ดูดีแต่ไม่ถูกต้องได้
    *   **Multimodality:** การอธิบายโมเดลที่ทำงานกับข้อมูลหลายประเภทพร้อมกัน (เช่น ข้อความและภาพ) ยังคงเป็นความท้าทายที่ซับซ้อน

---

## **3. ข้อพิจารณาด้านจริยธรรมและความยุติธรรมใน XAI**
การนำ XAI ไปใช้งานมีความเชื่อมโยงโดยตรงกับข้อพิจารณาด้านจริยธรรมและความยุติธรรม

*   **3.1. การตรวจจับความลำเอียงและ 'Fairwashing':** XAI เป็นเครื่องมือวินิจฉัยที่ทรงพลังในการเปิดโปงอคติที่แฝงอยู่ในโมเดล (เช่น การให้น้ำหนักกับคุณลักษณะที่ละเอียดอ่อนอย่างเพศหรือเชื้อชาติ) อย่างไรก็ตาม มีความเสี่ยงที่จะเกิด **"Fairwashing"** ซึ่งคือการใช้คำอธิบายที่ดูสมเหตุสมผลแต่ไม่สมบูรณ์มาสร้างความชอบธรรมให้กับการตัดสินใจที่ไม่เป็นธรรม [7]
*   **3.2. ความโปร่งใส (Transparency) กับความเป็นส่วนตัว (Privacy):** การให้คำอธิบายที่ละเอียดเกินไปอาจเปิดเผยข้อมูลที่ละเอียดอ่อนเกี่ยวกับบุคคลในชุดข้อมูลฝึกสอนโดยไม่ได้ตั้งใจ ทำให้เกิดความขัดแย้งระหว่างความโปร่งใสของโมเดลและความเป็นส่วนตัวของข้อมูล
*   **3.3. ความสามารถในการตีความที่จำเพาะต่อผู้รับสาร:** ประสิทธิผลของคำอธิบายขึ้นอยู่กับบริบทและความเชี่ยวชาญของผู้มีส่วนได้ส่วนเสีย คำอธิบายที่เหมาะสำหรับนักวิทยาศาสตร์ข้อมูลอาจไม่สามารถเข้าใจได้โดยผู้ใช้ปลายทาง ซึ่งจำเป็นต้องมีการออกแบบคำอธิบายที่เหมาะสมกับแต่ละกลุ่ม
*   **3.4. การเชื่อมั่นมากเกินไปและอคติจากการทำงานอัตโนมัติ (Automation Bias):** คำอธิบายที่ดูน่าเชื่อถือจาก XAI อาจทำให้เกิดความไว้วางใจในผลลัพธ์ของโมเดลมากเกินไป แม้ว่าโมเดลจะทำงานผิดพลาด อาจนำไปสู่ "อคติจากการทำงานอัตโนมัติ" ที่ผู้ใช้งานที่เป็นมนุษย์ละเลยการตรวจสอบคำแนะนำของ AI
*   **3.5. ความรับผิดชอบ (Accountability) และสิทธิในการเรียกร้อง (Recourse):** XAI เป็นรากฐานสำคัญในการสร้างความรับผิดชอบในระบบอัตโนมัติ โดยเป็นหลักฐานที่ตรวจสอบได้สำหรับการตัดสินใจ และผ่านเทคนิคอย่าง Counterfactuals ก็สามารถเสนอแนวทางให้บุคคลสามารถโต้แย้งหรือเรียกร้องการเยียวยาจากผลกระทบเชิงลบของอัลกอริทึมได้ ซึ่งสอดคล้องกับหลักการกำกับดูแล เช่น **"สิทธิในการได้รับคำอธิบาย" (Right to Explanation) ของ GDPR** [6]

---

## **4. บทสรุป**

ระเบียบวิธีของ XAI ที่ได้ทบทวนไปนี้เป็นชุดเครื่องมือที่แข็งแกร่งในการเพิ่มความโปร่งใสของระบบการเรียนรู้ของเครื่อง ขณะที่เทคนิคดั้งเดิมอย่าง LIME และ SHAP ให้ความยืดหยุ่นแบบ `model-agnostic` **แนวทางใหม่ๆ เช่น Attention visualization และ Chain-of-Thought ก็ได้เข้ามาตอบโจทย์ความท้าทายในการทำความเข้าใจ LLMs มากขึ้น** อย่างไรก็ตาม การประยุกต์ใช้เครื่องมือเหล่านี้ต้องควบคู่ไปกับการประเมินข้อจำกัดโดยธรรมชาติของมันและภูมิทัศน์ทางจริยธรรมในวงกว้าง เพื่อให้แน่ใจว่าการพัฒนา AI เป็นไปอย่างมีความรับผิดชอบ ยุติธรรม และน่าเชื่อถือ

---

## **เอกสารอ้างอิง**
[1] M. T. Ribeiro, S. Singh, and C. Guestrin, "Why Should I Trust You?": Explaining the Predictions of Any Classifier," in *Proc. 22nd ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining*, San Francisco, CA, USA, 2016, pp. 1135–1144.

[2] S. M. Lundberg and S. -I. Lee, "A Unified Approach to Interpreting Model Predictions," in *Advances in Neural Information Processing Systems 30 (NIPS 2017)*, 2017, pp. 4765–4774.

[3] J. H. Friedman, "Greedy Function Approximation: A Gradient Boosting Machine," *The Annals of Statistics*, vol. 29, no. 5, pp. 1189–1232, Oct. 2001.

[4] A. Goldstein, A. Kapelner, J. Bleich, and E. Pitkin, "Peeking Inside the Black Box: Visualizing Statistical Learning with Plots of Individual Conditional Expectation," *Journal of Computational and Graphical Statistics*, vol. 24, no. 1, pp. 44–65, 2015.

[5] R. R. Selvaraju et al., "Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization," in *Proc. IEEE Int. Conf. Computer Vision (ICCV)*, Venice, Italy, 2017, pp. 618–626.

[6] S. Wachter, B. Mittelstadt, and C. Russell, "Counterfactual Explanations Without Opening the Black Box: A Algorithmic and Auditing Perspective," *Harvard Journal of Law & Technology*, vol. 31, no. 2, pp. 841–887, 2018.

[7] U. Aïvodji, H. A. Asonze, G. F. H. T. de Souza, M. H. F. de Medeiros, and S. Gambs, "Fairwashing: the Risk of Explanation," in *Proc. 36th Int. Conf. Machine Learning (ICML)*, Long Beach, CA, USA, 2019, pp. 165-174.

[8] A. Vaswani et al., "Attention Is All You Need," in *Advances in Neural Information Processing Systems 30 (NIPS 2017)*, 2017, pp. 5998–6008.

[9] J. Wei et al., "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models," in *Advances in Neural Information Processing Systems 35 (NeurIPS 2022)*, 2022, pp. 24824–24837.

[10] S. Jain and B. C. Wallace, "Attention is not Explanation," in *Proc. Conf. of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT)*, 2019, pp. 3543–3556.